Summary of all Tests
-------------------
Core performance metrics:
1- Accuracy
2- F1-score
3- Classification report 
4- Confusion matrix
5- ROC-AUC
6- Per-class accuracy
Feature analysis:
7- Feature importance
8- Permutation importance
9- Feature ablation/usefulness
Model robustness & reliability:
10- Overfitting check
11- Class imbalance/distribution
12- Prediction confidence
13- Noise sensitivity
14- Stability test:
Error & unseen label analysis
15- Error analysis: Inspect misclassified samples
16- Unknown/unseen label handling


======================================
ML Model Evaluation Guide
======================================
--------------------------------------
1. Overall Accuracy:
--------------------------------------
Definition:
- Percentage of predictions that were correct overall

Interpretation:
- >= 90% : Excellent
- 80% - 89% : Good
- 70% - 79% : Moderate
- < 70% : Low

Suggestions:
- Check class balance.
- Add more data or features.
- Try other algorithms or hyperparameter tuning.

--------------------------------------
2. Weighted F1-Score:
--------------------------------------
Definition:
- Weighted balance of precision and recall for all classes (good for imbalanced datasets)

Interpretation:
- High value (~1.0) means the model performs well across all classes.
- Low value (~0.0) indicates poor precision or recall in one or more classes.

Suggestions:
- Focus on classes with low F1-scores.
- Consider rebalancing dataset or using class weights.

--------------------------------------
3. Classification Report:
--------------------------------------
Definition:
- Detailed metrics (precision, recall, F1-score, support) for each class.
- Highlights which classes are predicted well or poorly.

Interpretation:
- High precision → few false positives.
- High recall → few false negatives.
- Low numbers → class may need more data or features.

Suggestions:
- Focus on improving weak classes.
- Adjust sampling or feature engineering.

--------------------------------------
4. Confusion Matrix:
--------------------------------------
Definition:
- Table showing true vs predicted labels.
- Diagonal = correct predictions; off-diagonal = misclassifications.

Interpretation:
- Most predictions on diagonal → correctly classified
- Off-diagonal errors → identify patterns of misclassification

Suggestions:
- Improve features for misclassified classes.
- Consider ensemble methods or class-specific tuning.
- Adjust features or preprocessing to reduce errors.

--------------------------------------
5. ROC-AUC Score (Multi-class):
--------------------------------------
Definition:
- Measures model’s ability to distinguish classes using predicted probabilities.
- Higher value indicates better separation between classes.

Interpretation:
- 1.00       : Perfect classifier
- 0.90 - 0.99: Excellent
- 0.80 - 0.89: Good
- 0.70 - 0.79: Fair
- ~0.50      : No better than random guessing

Suggestions:
- Check probability calibration.
- Improve features or model to better separate classes.

--------------------------------------
6. Per-Class Accuracy:
--------------------------------------
Definition:
- Accuracy calculated separately for each class.
- Shows how well the model performs on each specific class in isolation.

Interpretation:
- High → class predicted well
- Low → class underrepresented or features insufficient

Suggestions:
- Collect more samples for weak classes.
- Feature engineering to improve class separability.

--------------------------------------
7. Feature Importance:
--------------------------------------
Definition:
- Shows which features contribute most to model decisions.

Interpretation:
- High score → critical feature
- Low score → optional or redundant feature

Suggestions:
- Focus on top features for insights.
- Consider removing low-impact features to simplify model.

--------------------------------------
8. Permutation Importance:
--------------------------------------
Definition:
- Measures drop in accuracy when a feature is randomly shuffled.

Interpretation:
- Large drop → feature is critical
- Small drop → feature less important

Suggestions:
- Focus model improvement on important features.
- Features with near-zero permutation importance can likely be removed.
- Negative values suggest the feature may be adding noise.

--------------------------------------
9. Feature Ablation / Usefulness:
--------------------------------------
Definition:
- Each feature is removed and the model is retrained from scratch
- Accuracy is compared to the baseline to measure the feature's contribution

Interpretation:
- Large drop -> feature is highly useful
- Small/no drop → feature may be redundant or noisy

Suggestions:
- Remove unnecessary features to simplify model.
- Keep critical features for stability.
Note: Ablation is computationally expensive, only repeat when making significant data changes*

--------------------------------------
10. Overfitting Check:
--------------------------------------
Definition:
- Compare train vs test accuracy to detect overfitting.
- Overfitting: model memorises training data but fails to generalise to new data

Interpretation:
- Train >> Test → Overfitting
- Train ≈ Test → Generalizes well
Thresholds (Train - Test gap):
- < 5%  : Acceptable
- 5-10% : Slight overfitting; consider regularisation
- > 10% : Significant overfitting; action needed

Suggestions:
- Reduce model complexity
- Add regularization
- Collect more training data

--------------------------------------
11. Class Imbalance / Distribution:
--------------------------------------
Definition:
- Shows the number of samples per class in the dataset

Interpretation:
- Large imbalance → model may favor majority class

Suggestions:
- Resample (oversample/undersample)
- Use class weights
- Collect more samples for minority classes

--------------------------------------
12. Prediction Confidence:
--------------------------------------
Definition:
- Maximum predicted probability per sample (model's certainty in its prediction)

Interpretation:
- Most samples in High (0.75-1.0)  : Model is confident and likely reliable
- Many samples in Low/Very Low     : Model is uncertain; predictions may be unreliable
- Overconfidence (all near 1.0)    : May indicate overfitting or poor calibration

Suggestions:
- Investigate low-confidence samples (- Apply a confidence threshold; flag low-confidence predictions for manual review.)
- Low confidence on a specific class may indicate insufficient training data for that class.
- Use probability calibration if confidence is poorly calibrated.

--------------------------------------
13. Noise Sensitivity:
--------------------------------------
Definition:
- Tests model robustness by applying small multiplicative noise to features (0.5%, 1%, 5%)
- Measures accuracy on noisy input vs clean input

Interpretation:
- Stable across noise levels  → robust model
- Big drop → fragile/ sensitive to small input changes, Fragile/overfitted model

Thresholds (accuracy drop vs baseline):
- < 2%  : Robust
- 2-5%  : Slightly sensitive; monitor in production
- > 5%  : Fragile; consider regularisation or data augmentation

Suggestions:
- Consider data augmentation
- Add regularization or simpler model
- If sensitive to noise, increase training data diversity.
- Review feature scaling; unscaled features amplify noise effects.

--------------------------------------
14. Stability Test:
--------------------------------------
Definition:
 Runs multiple train/test splits and records accuracy each time
- Measures consistency of model performance across different data splits
What it is: List of accuracies + mean.

Interpretation:
- Consistent → stable and reliable model
- Wide variation → model sensitive to training data
Thresholds (standard deviation of scores):
- < 2%  : Stable
- 2-5%  : Moderate variance; acceptable
- > 5%  : Unstable; investigate data or model

Suggestions:
- Collect more data (Increase dataset size to reduce variance.)
- Improve features or model robustness
- High instability may indicate data leakage or inconsistent labelling.

--------------------------------------
15. Error Analysis:
--------------------------------------
Definition:
- Inspect misclassified samples to understand mistakes with true vs predicted labels.

Interpretation:
- Clustered misclassifications  : Systematic model confusion (e.g. two classes too similar)
- Scattered misclassifications  : Random noise in data; harder to fix

Suggestions:
- Feature engineering
- Collect more data for misclassified classes

--------------------------------------
16. Unknown / Unseen Label Handling:
--------------------------------------
Definition:
- Tests whether the model raises an appropriate error when given a class label it has never seen before

Interpretation:
- Model correctly raises error → safe deployment
- No error / wrong behaviour  → Risky; unseen labels may be silently misclassified

Suggestions:
- Consider retraining if new classes expected
- Handle unknown labels in preprocessing
